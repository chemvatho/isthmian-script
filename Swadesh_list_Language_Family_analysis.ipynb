{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN+mJQOPj7BtN64Sd5c7wMj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chemvatho/isthmian-script/blob/main/Swadesh_list_Language_Family_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Isthmus Script Languages Dataset Parser\n",
        "========================================\n",
        "Parses comparative wordlist data for Mesoamerican languages\n",
        "relevant to the Cologne Isthmus Script decipherment project.\n",
        "\n",
        "Language families represented:\n",
        "- Otomanguean: Otomian, Trique, Mixtec, Zapotec, Mazahua\n",
        "- Mixe-Zoquean: Popoluca\n",
        "- Totonacan: Totonac\n",
        "- Isolate: Huave\n",
        "- Chontal (Tequistlatecan)\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "import unicodedata\n",
        "\n",
        "# Read the CSV with proper handling of multi-line cells\n",
        "df = pd.read_csv('Isthmus_script_languages.csv', encoding='utf-8')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ISTHMUS SCRIPT LANGUAGES DATASET - PARSING REPORT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Display column structure\n",
        "print(\"\\n1. COLUMN STRUCTURE\")\n",
        "print(\"-\" * 40)\n",
        "for i, col in enumerate(df.columns):\n",
        "    print(f\"  [{i}] {col}\")\n",
        "\n",
        "# Get the dialect/variety info from row 0\n",
        "print(\"\\n2. LANGUAGE VARIETIES (from header row)\")\n",
        "print(\"-\" * 40)\n",
        "varieties = df.iloc[0].to_dict()\n",
        "for col, variety in varieties.items():\n",
        "    if pd.notna(variety) and variety.strip():\n",
        "        print(f\"  {col}: {variety}\")\n",
        "\n",
        "# Remove the variety row and reset\n",
        "df_clean = df.iloc[1:].copy()\n",
        "df_clean.columns = df.columns\n",
        "df_clean = df_clean.reset_index(drop=True)\n",
        "\n",
        "# Rename columns for clarity\n",
        "column_mapping = {\n",
        "    '№': 'entry_num',\n",
        "    'English': 'gloss',\n",
        "    'Otomian languages': 'otomian',\n",
        "    'Trique languages': 'trique',\n",
        "    'Mixtec languages': 'proto_mixtec',\n",
        "    'Unnamed: 4': 'mixtec_tlaxiaco',\n",
        "    'Zapotec languages': 'zapotec_isthmus',\n",
        "    'Unnamed: 6': 'zapotec_xhon',\n",
        "    'Mazahua languages': 'mazahua',\n",
        "    'Totonac languages': 'totonac',\n",
        "    'Popoluca languages': 'popoluca',\n",
        "    'Huave languages': 'huave',\n",
        "    'Chontal languages': 'chontal'\n",
        "}\n",
        "\n",
        "# Apply mapping where columns exist\n",
        "new_columns = []\n",
        "for col in df_clean.columns:\n",
        "    if col in column_mapping:\n",
        "        new_columns.append(column_mapping[col])\n",
        "    else:\n",
        "        new_columns.append(col)\n",
        "df_clean.columns = new_columns\n",
        "\n",
        "print(\"\\n3. DATASET STATISTICS\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"  Total entries (glosses): {len(df_clean)}\")\n",
        "print(f\"  Languages/varieties: {len(df_clean.columns) - 2}\")  # minus entry_num and gloss\n",
        "\n",
        "# Count non-empty cells per language\n",
        "print(\"\\n4. DATA COMPLETENESS BY LANGUAGE\")\n",
        "print(\"-\" * 40)\n",
        "language_cols = [c for c in df_clean.columns if c not in ['entry_num', 'gloss']]\n",
        "\n",
        "completeness = {}\n",
        "for col in language_cols:\n",
        "    non_empty = df_clean[col].notna().sum()\n",
        "    non_empty_actual = df_clean[col].apply(lambda x: bool(str(x).strip()) if pd.notna(x) else False).sum()\n",
        "    completeness[col] = non_empty_actual\n",
        "    pct = (non_empty_actual / len(df_clean)) * 100\n",
        "    print(f\"  {col:20s}: {non_empty_actual:3d} entries ({pct:5.1f}%)\")\n",
        "\n",
        "# Language family groupings\n",
        "print(\"\\n5. LANGUAGE FAMILY GROUPINGS\")\n",
        "print(\"-\" * 40)\n",
        "families = {\n",
        "    'Otomanguean': {\n",
        "        'Oto-Pamean': ['otomian', 'mazahua'],\n",
        "        'Mixtecan': ['trique', 'proto_mixtec', 'mixtec_tlaxiaco'],\n",
        "        'Zapotecan': ['zapotec_isthmus', 'zapotec_xhon']\n",
        "    },\n",
        "    'Mixe-Zoquean': ['popoluca'],\n",
        "    'Totonacan': ['totonac'],\n",
        "    'Huavean (isolate)': ['huave'],\n",
        "    'Tequistlatecan': ['chontal']\n",
        "}\n",
        "\n",
        "for family, members in families.items():\n",
        "    print(f\"\\n  {family}:\")\n",
        "    if isinstance(members, dict):\n",
        "        for branch, langs in members.items():\n",
        "            print(f\"    └─ {branch}: {', '.join(langs)}\")\n",
        "    else:\n",
        "        print(f\"    └─ {', '.join(members)}\")\n",
        "\n",
        "# Extract and analyze Proto-Mixtec forms\n",
        "print(\"\\n6. PROTO-MIXTEC RECONSTRUCTIONS\")\n",
        "print(\"-\" * 40)\n",
        "proto_mixtec_forms = df_clean[['gloss', 'proto_mixtec']].dropna(subset=['proto_mixtec'])\n",
        "proto_mixtec_forms = proto_mixtec_forms[proto_mixtec_forms['proto_mixtec'].str.strip() != '']\n",
        "\n",
        "print(f\"  Total Proto-Mixtec forms: {len(proto_mixtec_forms)}\")\n",
        "print(\"\\n  Sample Proto-Mixtec reconstructions:\")\n",
        "for _, row in proto_mixtec_forms.head(15).iterrows():\n",
        "    gloss = str(row['gloss']).replace('\\n', ' ')[:25]\n",
        "    form = str(row['proto_mixtec'])\n",
        "    print(f\"    '{gloss:25s}' : {form}\")\n",
        "\n",
        "# Phoneme inventory extraction\n",
        "print(\"\\n7. PHONEME/GRAPHEME INVENTORY ANALYSIS\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "def extract_segments(text):\n",
        "    \"\"\"Extract potential phonemic segments from transcriptions\"\"\"\n",
        "    if pd.isna(text) or not str(text).strip():\n",
        "        return set()\n",
        "\n",
        "    text = str(text)\n",
        "    # Remove common annotations\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)  # Remove parenthetical notes\n",
        "    text = re.sub(r'\\*', '', text)  # Remove reconstruction markers\n",
        "    text = re.sub(r'[0-9]+', '', text)  # Remove tone numbers for now\n",
        "    text = re.sub(r'[,;/\\-\\s]+', ' ', text)  # Normalize separators\n",
        "\n",
        "    # Get unique characters (simplified)\n",
        "    chars = set()\n",
        "    for char in text:\n",
        "        if unicodedata.category(char)[0] == 'L':  # Letters only\n",
        "            chars.add(char.lower())\n",
        "    return chars\n",
        "\n",
        "# Collect segments per language\n",
        "language_inventories = {}\n",
        "for col in language_cols:\n",
        "    all_segments = set()\n",
        "    for val in df_clean[col]:\n",
        "        all_segments.update(extract_segments(val))\n",
        "    language_inventories[col] = sorted(all_segments)\n",
        "\n",
        "print(\"\\n  Unique graphemes per language:\")\n",
        "for lang, segments in language_inventories.items():\n",
        "    print(f\"    {lang:20s}: {len(segments):3d} unique characters\")\n",
        "\n",
        "# Semantic domain analysis\n",
        "print(\"\\n8. SEMANTIC DOMAIN CATEGORIZATION\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Define semantic domains based on Swadesh list categories\n",
        "semantic_domains = {\n",
        "    'pronouns': ['I', 'you', 'he', 'we', 'they', 'this', 'that'],\n",
        "    'interrogatives': ['who', 'what', 'where', 'when', 'how'],\n",
        "    'body_parts': ['head', 'eye', 'ear', 'nose', 'mouth', 'tooth', 'tongue',\n",
        "                   'hand', 'foot', 'heart', 'blood', 'bone', 'skin', 'hair',\n",
        "                   'belly', 'neck', 'knee', 'finger', 'leg', 'breast', 'liver',\n",
        "                   'back', 'fingernail', 'wing', 'tail', 'feather', 'horn'],\n",
        "    'kinship': ['mother', 'father', 'wife', 'husband', 'child', 'man', 'woman'],\n",
        "    'animals': ['dog', 'fish', 'bird', 'snake', 'louse', 'worm', 'animal',\n",
        "                'deer', 'mouse', 'rabbit', 'eagle', 'spider'],\n",
        "    'nature': ['sun', 'moon', 'star', 'water', 'rain', 'river', 'lake', 'sea',\n",
        "               'fire', 'stone', 'sand', 'earth', 'cloud', 'sky', 'wind',\n",
        "               'mountain', 'tree', 'forest', 'leaf', 'root', 'flower', 'grass'],\n",
        "    'colors': ['red', 'green', 'yellow', 'white', 'black'],\n",
        "    'numerals': ['one', 'two', 'three', 'four', 'five', 'six', 'seven',\n",
        "                 'eight', 'nine', 'ten'],\n",
        "    'verbs': ['eat', 'drink', 'sleep', 'die', 'kill', 'walk', 'come', 'go',\n",
        "              'see', 'hear', 'know', 'give', 'say', 'burn', 'fly', 'swim',\n",
        "              'sit', 'stand', 'lie', 'bite', 'suck', 'spit'],\n",
        "    'adjectives': ['big', 'small', 'long', 'short', 'good', 'bad', 'new',\n",
        "                   'old', 'hot', 'cold', 'wet', 'dry', 'full', 'round']\n",
        "}\n",
        "\n",
        "# Categorize entries\n",
        "domain_coverage = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for _, row in df_clean.iterrows():\n",
        "    gloss = str(row['gloss']).lower().replace('\\n', ' ')\n",
        "\n",
        "    for domain, keywords in semantic_domains.items():\n",
        "        for keyword in keywords:\n",
        "            if keyword.lower() in gloss:\n",
        "                for lang in language_cols:\n",
        "                    if pd.notna(row[lang]) and str(row[lang]).strip():\n",
        "                        domain_coverage[domain][lang] += 1\n",
        "                break\n",
        "\n",
        "print(\"\\n  Entries per semantic domain:\")\n",
        "for domain in semantic_domains.keys():\n",
        "    total = sum(domain_coverage[domain].values())\n",
        "    if total > 0:\n",
        "        print(f\"    {domain:15s}: {total // len(language_cols):3d} avg entries\")\n",
        "\n",
        "# Detect Spanish loanwords\n",
        "print(\"\\n9. SPANISH LOANWORD DETECTION\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "spanish_markers = ['Spanish', 'spanish', '(Spanish)']\n",
        "loanwords = defaultdict(list)\n",
        "\n",
        "for _, row in df_clean.iterrows():\n",
        "    gloss = str(row['gloss']).replace('\\n', ' ')\n",
        "    for lang in language_cols:\n",
        "        val = str(row[lang]) if pd.notna(row[lang]) else ''\n",
        "        for marker in spanish_markers:\n",
        "            if marker in val:\n",
        "                loanwords[lang].append(gloss)\n",
        "                break\n",
        "\n",
        "print(\"\\n  Spanish loans marked per language:\")\n",
        "for lang, loans in sorted(loanwords.items(), key=lambda x: -len(x[1])):\n",
        "    if loans:\n",
        "        print(f\"    {lang:20s}: {len(loans):2d} marked loans\")\n",
        "        for loan in loans[:3]:\n",
        "            print(f\"      - {loan[:40]}\")\n",
        "\n",
        "# Create structured output\n",
        "print(\"\\n10. GENERATING STRUCTURED DATA FILES\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Create a clean JSON export\n",
        "output_data = {\n",
        "    'metadata': {\n",
        "        'title': 'Isthmus Script Languages Comparative Wordlist',\n",
        "        'total_entries': int(len(df_clean)),\n",
        "        'languages': language_cols,\n",
        "        'families': families\n",
        "    },\n",
        "    'completeness': {k: int(v) for k, v in completeness.items()},\n",
        "    'entries': []\n",
        "}\n",
        "\n",
        "for _, row in df_clean.iterrows():\n",
        "    entry = {\n",
        "        'id': row['entry_num'],\n",
        "        'gloss': str(row['gloss']).replace('\\n', ' ') if pd.notna(row['gloss']) else '',\n",
        "        'forms': {}\n",
        "    }\n",
        "    for lang in language_cols:\n",
        "        if pd.notna(row[lang]) and str(row[lang]).strip():\n",
        "            entry['forms'][lang] = str(row[lang])\n",
        "    output_data['entries'].append(entry)\n",
        "\n",
        "# Save JSON\n",
        "with open('isthmus_parsed.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
        "print(\"  ✓ Saved: isthmus_parsed.json\")\n",
        "\n",
        "# Save clean CSV\n",
        "df_clean.to_csv('isthmus_cleaned.csv', index=False, encoding='utf-8')\n",
        "print(\"  ✓ Saved: isthmus_cleaned.csv\")\n",
        "\n",
        "# Create a Proto-Mixtec focused file\n",
        "mixtec_cols = ['entry_num', 'gloss', 'proto_mixtec', 'trique']\n",
        "# Add the Tlaxiaco column if it exists\n",
        "if 'Unnamed: 5' in df_clean.columns:\n",
        "    mixtec_cols.append('Unnamed: 5')\n",
        "proto_mixtec_data = df_clean[mixtec_cols].copy()\n",
        "proto_mixtec_data = proto_mixtec_data.dropna(subset=['proto_mixtec'])\n",
        "proto_mixtec_data.to_csv('proto_mixtec_forms.csv', index=False, encoding='utf-8')\n",
        "print(\"  ✓ Saved: proto_mixtec_forms.csv\")\n",
        "\n",
        "# Summary statistics table\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PARSING COMPLETE - SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\"\"\n",
        "  Dataset: Isthmus Script Languages Comparative Wordlist\n",
        "\n",
        "  Entries:     {len(df_clean)} glosses (Swadesh-style list)\n",
        "  Languages:   {len(language_cols)} varieties across 5 families\n",
        "\n",
        "  Best documented:\n",
        "    - Zapotec (Isthmus): {completeness.get('zapotec_isthmus', 0)} entries\n",
        "    - Otomian:          {completeness.get('otomian', 0)} entries\n",
        "    - Chontal:          {completeness.get('chontal', 0)} entries\n",
        "\n",
        "  Proto-forms available:\n",
        "    - Proto-Mixtec:     {len(proto_mixtec_forms)} reconstructions\n",
        "\n",
        "  Relevance to Cologne Project:\n",
        "    ✓ Otomanguean data (for Proto-Otomanguean reconstruction)\n",
        "    ✓ Popoluca data (for Proto-Mixe-Zoquean comparison)\n",
        "    ✓ Multiple Zapotecan varieties (Isthmus region focus)\n",
        "\n",
        "  Output files:\n",
        "    - isthmus_parsed.json   (structured data)\n",
        "    - isthmus_cleaned.csv   (cleaned tabular data)\n",
        "    - proto_mixtec_forms.csv (Proto-Mixtec focus)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "LA6M1NaeF0r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Advanced Linguistic Analysis: Sound Correspondences & Cognate Detection\n",
        "========================================================================\n",
        "For the Cologne Isthmus Script Project application\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import json\n",
        "from collections import defaultdict, Counter\n",
        "from itertools import combinations\n",
        "import unicodedata\n",
        "\n",
        "# Load the parsed data\n",
        "with open('isthmus_parsed.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ADVANCED LINGUISTIC ANALYSIS\")\n",
        "print(\"Sound Correspondences & Potential Cognate Detection\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DETAILED PHONEME INVENTORY EXTRACTION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"1. PHONEME INVENTORY BY LANGUAGE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def normalize_form(text):\n",
        "    \"\"\"Normalize a linguistic form for comparison\"\"\"\n",
        "    if not text:\n",
        "        return ''\n",
        "    # Remove parenthetical notes\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "    # Remove reconstruction asterisks\n",
        "    text = re.sub(r'\\*', '', text)\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def extract_phonemes(text):\n",
        "    \"\"\"Extract phonemic segments with diacritics preserved\"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    text = normalize_form(text)\n",
        "    segments = []\n",
        "\n",
        "    # Common digraphs/trigraphs in Mesoamerican orthographies\n",
        "    digraphs = ['ts', 'tz', 'ch', 'dz', 'gu', 'qu', 'hu', 'ng', 'ñh', 'xh',\n",
        "                'nd', 'mb', 'nh', 'th', 'ph', 'kh', 'tx', 'dj', 'nz', 'ny']\n",
        "\n",
        "    i = 0\n",
        "    while i < len(text):\n",
        "        # Skip non-letter characters but note tone markers\n",
        "        if not text[i].isalpha() and text[i] not in \"'ʔʼˀ\":\n",
        "            i += 1\n",
        "            continue\n",
        "\n",
        "        # Check for digraphs first\n",
        "        found_digraph = False\n",
        "        for dg in digraphs:\n",
        "            if text[i:i+len(dg)].lower() == dg:\n",
        "                # Include any following diacritics\n",
        "                end = i + len(dg)\n",
        "                while end < len(text) and unicodedata.category(text[end]).startswith('M'):\n",
        "                    end += 1\n",
        "                segments.append(text[i:end].lower())\n",
        "                i = end\n",
        "                found_digraph = True\n",
        "                break\n",
        "\n",
        "        if not found_digraph:\n",
        "            # Single character with potential combining diacritics\n",
        "            char = text[i]\n",
        "            end = i + 1\n",
        "            while end < len(text) and unicodedata.category(text[end]).startswith('M'):\n",
        "                end += 1\n",
        "            segments.append(text[i:end].lower())\n",
        "            i = end\n",
        "\n",
        "    return segments\n",
        "\n",
        "# Build phoneme inventories\n",
        "phoneme_inventories = defaultdict(Counter)\n",
        "\n",
        "for entry in data['entries']:\n",
        "    for lang, form in entry['forms'].items():\n",
        "        if form:\n",
        "            # Handle multiple forms separated by comma\n",
        "            for variant in form.split(','):\n",
        "                segments = extract_phonemes(variant)\n",
        "                for seg in segments:\n",
        "                    phoneme_inventories[lang][seg] += 1\n",
        "\n",
        "# Display inventories\n",
        "print(\"\\nConsonant and vowel inventories (frequency > 5):\\n\")\n",
        "\n",
        "for lang in sorted(phoneme_inventories.keys()):\n",
        "    inventory = phoneme_inventories[lang]\n",
        "    common_segments = [seg for seg, count in inventory.most_common(50) if count >= 5]\n",
        "\n",
        "    # Rough categorization\n",
        "    vowels = [s for s in common_segments if s[0] in 'aeiouɨɛɔəʌæœøüöä']\n",
        "    consonants = [s for s in common_segments if s not in vowels]\n",
        "\n",
        "    print(f\"\\n{lang.upper()}\")\n",
        "    print(f\"  Vowels ({len(vowels)}):     {' '.join(sorted(vowels))}\")\n",
        "    print(f\"  Consonants ({len(consonants)}): {' '.join(sorted(consonants)[:25])}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. SOUND CORRESPONDENCE DETECTION (Otomanguean focus)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"2. SOUND CORRESPONDENCES WITHIN OTOMANGUEAN\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Focus on Otomanguean languages for internal comparison\n",
        "otomanguean_langs = ['otomian', 'trique', 'proto_mixtec', 'Unnamed: 5',\n",
        "                      'zapotec_isthmus', 'Unnamed: 7', 'mazahua']\n",
        "\n",
        "# Build correspondence pairs\n",
        "correspondence_pairs = defaultdict(Counter)\n",
        "\n",
        "for entry in data['entries']:\n",
        "    forms = entry['forms']\n",
        "    gloss = entry['gloss']\n",
        "\n",
        "    # Get initial segments for each language\n",
        "    initial_segments = {}\n",
        "    for lang in otomanguean_langs:\n",
        "        if lang in forms and forms[lang]:\n",
        "            segs = extract_phonemes(forms[lang].split(',')[0])\n",
        "            if segs:\n",
        "                initial_segments[lang] = segs[0]\n",
        "\n",
        "    # Compare pairs\n",
        "    for lang1, lang2 in combinations(otomanguean_langs, 2):\n",
        "        if lang1 in initial_segments and lang2 in initial_segments:\n",
        "            seg1 = initial_segments[lang1]\n",
        "            seg2 = initial_segments[lang2]\n",
        "            correspondence_pairs[(lang1, lang2)][(seg1, seg2)] += 1\n",
        "\n",
        "print(\"\\nInitial consonant correspondences (top patterns):\\n\")\n",
        "\n",
        "# Focus on key pairs\n",
        "key_pairs = [\n",
        "    ('otomian', 'zapotec_isthmus'),\n",
        "    ('proto_mixtec', 'zapotec_isthmus'),\n",
        "    ('trique', 'Unnamed: 5'),\n",
        "    ('otomian', 'mazahua'),\n",
        "]\n",
        "\n",
        "for pair in key_pairs:\n",
        "    if pair in correspondence_pairs:\n",
        "        corr = correspondence_pairs[pair]\n",
        "        print(f\"\\n{pair[0]} ↔ {pair[1]}:\")\n",
        "        top_corr = corr.most_common(8)\n",
        "        for (seg1, seg2), count in top_corr:\n",
        "            if count >= 2:\n",
        "                print(f\"    {seg1:6s} : {seg2:6s}  ({count} instances)\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. POTENTIAL COGNATE SETS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"3. POTENTIAL COGNATE SETS (Cross-family)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def phonetic_similarity(form1, form2):\n",
        "    \"\"\"Calculate rough phonetic similarity between two forms\"\"\"\n",
        "    if not form1 or not form2:\n",
        "        return 0\n",
        "\n",
        "    seg1 = set(extract_phonemes(form1))\n",
        "    seg2 = set(extract_phonemes(form2))\n",
        "\n",
        "    if not seg1 or not seg2:\n",
        "        return 0\n",
        "\n",
        "    intersection = len(seg1 & seg2)\n",
        "    union = len(seg1 | seg2)\n",
        "\n",
        "    return intersection / union if union > 0 else 0\n",
        "\n",
        "# Look for potential cognates between families\n",
        "print(\"\\nHigh-similarity forms between language families:\\n\")\n",
        "\n",
        "mixe_zoque = ['popoluca']\n",
        "totonacan = ['totonac']\n",
        "huavean = ['huave']\n",
        "chontal_fam = ['chontal']\n",
        "\n",
        "family_pairs = [\n",
        "    ('Otomanguean', otomanguean_langs, 'Mixe-Zoquean', mixe_zoque),\n",
        "    ('Otomanguean', otomanguean_langs, 'Totonacan', totonacan),\n",
        "    ('Mixe-Zoquean', mixe_zoque, 'Totonacan', totonacan),\n",
        "]\n",
        "\n",
        "potential_cognates = []\n",
        "\n",
        "for entry in data['entries']:\n",
        "    forms = entry['forms']\n",
        "    gloss = entry['gloss']\n",
        "\n",
        "    # Check Popoluca vs Otomanguean (potential loans or cognates)\n",
        "    if 'popoluca' in forms:\n",
        "        pop_form = forms['popoluca']\n",
        "        for oto_lang in ['zapotec_isthmus', 'otomian', 'proto_mixtec']:\n",
        "            if oto_lang in forms:\n",
        "                oto_form = forms[oto_lang]\n",
        "                sim = phonetic_similarity(pop_form, oto_form)\n",
        "                if sim > 0.4:\n",
        "                    potential_cognates.append({\n",
        "                        'gloss': gloss,\n",
        "                        'popoluca': pop_form,\n",
        "                        oto_lang: oto_form,\n",
        "                        'similarity': sim\n",
        "                    })\n",
        "\n",
        "# Display potential cognates/loans\n",
        "print(\"Popoluca ↔ Otomanguean (possible loans/cognates):\\n\")\n",
        "seen_glosses = set()\n",
        "for item in sorted(potential_cognates, key=lambda x: -x['similarity'])[:15]:\n",
        "    if item['gloss'] not in seen_glosses:\n",
        "        print(f\"  '{item['gloss']}'\")\n",
        "        print(f\"    Popoluca: {item.get('popoluca', 'N/A')}\")\n",
        "        for lang in ['zapotec_isthmus', 'otomian', 'proto_mixtec']:\n",
        "            if lang in item:\n",
        "                print(f\"    {lang}: {item[lang]}\")\n",
        "        print(f\"    Similarity: {item['similarity']:.2f}\")\n",
        "        print()\n",
        "        seen_glosses.add(item['gloss'])\n",
        "\n",
        "# ============================================================================\n",
        "# 4. PROTO-MIXTEC FORM ANALYSIS\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"4. PROTO-MIXTEC RECONSTRUCTION ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "proto_mixtec_entries = []\n",
        "for entry in data['entries']:\n",
        "    if 'proto_mixtec' in entry['forms']:\n",
        "        proto_mixtec_entries.append({\n",
        "            'gloss': entry['gloss'],\n",
        "            'proto_form': entry['forms']['proto_mixtec'],\n",
        "            'trique': entry['forms'].get('trique', ''),\n",
        "            'mixtec_daughter': entry['forms'].get('Unnamed: 5', '')\n",
        "        })\n",
        "\n",
        "print(f\"\\nTotal Proto-Mixtec reconstructions: {len(proto_mixtec_entries)}\")\n",
        "\n",
        "# Analyze proto-Mixtec phoneme patterns\n",
        "proto_phonemes = Counter()\n",
        "for entry in proto_mixtec_entries:\n",
        "    segs = extract_phonemes(entry['proto_form'])\n",
        "    for seg in segs:\n",
        "        proto_phonemes[seg] += 1\n",
        "\n",
        "print(\"\\nProto-Mixtec phoneme frequency:\")\n",
        "print(\"-\" * 40)\n",
        "for phoneme, count in proto_phonemes.most_common(25):\n",
        "    bar = '█' * (count // 2)\n",
        "    print(f\"  {phoneme:6s} : {count:3d} {bar}\")\n",
        "\n",
        "# Identify systematic sound changes Proto-Mixtec → daughter languages\n",
        "print(\"\\nSound changes: Proto-Mixtec → Tlaxiaco/Putla Mixtec\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "pm_to_daughter = defaultdict(Counter)\n",
        "for entry in proto_mixtec_entries:\n",
        "    pm_segs = extract_phonemes(entry['proto_form'])\n",
        "    da_segs = extract_phonemes(entry['mixtec_daughter'])\n",
        "\n",
        "    # Align by position (simplified)\n",
        "    for i, pm_seg in enumerate(pm_segs[:min(3, len(pm_segs))]):\n",
        "        if i < len(da_segs):\n",
        "            pm_to_daughter[pm_seg][da_segs[i]] += 1\n",
        "\n",
        "print(\"\\nInitial position correspondences:\")\n",
        "for pm_seg in sorted(pm_to_daughter.keys()):\n",
        "    correspondences = pm_to_daughter[pm_seg]\n",
        "    if correspondences.total() >= 3:\n",
        "        top_corr = correspondences.most_common(3)\n",
        "        corr_str = ', '.join([f\"{seg}({n})\" for seg, n in top_corr])\n",
        "        print(f\"  *{pm_seg} → {corr_str}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5. SEMANTIC FIELD ANALYSIS FOR RECONSTRUCTION\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"5. SEMANTIC FIELDS WITH BEST DATA COVERAGE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "semantic_coverage = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "# Define expanded semantic categories\n",
        "categories = {\n",
        "    'basic_vocabulary': ['I', 'you', 'we', 'this', 'that', 'what', 'who', 'not', 'all'],\n",
        "    'numerals': ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten'],\n",
        "    'body_parts': ['head', 'eye', 'ear', 'nose', 'mouth', 'hand', 'foot', 'heart', 'blood', 'bone'],\n",
        "    'nature': ['sun', 'moon', 'star', 'water', 'fire', 'stone', 'tree', 'earth', 'rain'],\n",
        "    'animals': ['dog', 'fish', 'bird', 'snake', 'deer'],\n",
        "    'kinship': ['mother', 'father', 'man', 'woman', 'child'],\n",
        "    'colors': ['red', 'white', 'black', 'green', 'yellow'],\n",
        "    'actions': ['eat', 'drink', 'sleep', 'die', 'see', 'hear', 'come', 'go']\n",
        "}\n",
        "\n",
        "for entry in data['entries']:\n",
        "    gloss_lower = entry['gloss'].lower()\n",
        "    num_forms = len(entry['forms'])\n",
        "\n",
        "    for category, keywords in categories.items():\n",
        "        for kw in keywords:\n",
        "            if kw in gloss_lower:\n",
        "                semantic_coverage[category]['total'] += 1\n",
        "                semantic_coverage[category]['forms'] += num_forms\n",
        "                break\n",
        "\n",
        "print(\"\\nSemantic field coverage (for comparative reconstruction):\\n\")\n",
        "print(f\"{'Category':<20s} {'Entries':>8s} {'Total Forms':>12s} {'Avg Forms/Entry':>16s}\")\n",
        "print(\"-\" * 60)\n",
        "for cat in categories.keys():\n",
        "    total = semantic_coverage[cat]['total']\n",
        "    forms = semantic_coverage[cat]['forms']\n",
        "    avg = forms / total if total > 0 else 0\n",
        "    print(f\"{cat:<20s} {total:>8d} {forms:>12d} {avg:>16.1f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# 6. EXPORT ANALYSIS DATA\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"6. EXPORTING ANALYSIS DATA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create analysis summary\n",
        "analysis_summary = {\n",
        "    'phoneme_inventories': {lang: dict(inv.most_common(50))\n",
        "                           for lang, inv in phoneme_inventories.items()},\n",
        "    'proto_mixtec_phonemes': dict(proto_phonemes.most_common()),\n",
        "    'potential_cognates': potential_cognates[:20],\n",
        "    'semantic_coverage': {k: dict(v) for k, v in semantic_coverage.items()}\n",
        "}\n",
        "\n",
        "with open('linguistic_analysis.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(analysis_summary, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"  ✓ Saved: linguistic_analysis.json\")\n",
        "\n",
        "# Create a cognate candidates CSV for manual review\n",
        "cognate_df = pd.DataFrame(potential_cognates)\n",
        "cognate_df.to_csv('cognate_candidates.csv', index=False, encoding='utf-8')\n",
        "print(\"  ✓ Saved: cognate_candidates.csv\")\n",
        "\n",
        "# Proto-Mixtec with daughter forms\n",
        "pm_df = pd.DataFrame(proto_mixtec_entries)\n",
        "pm_df.to_csv('proto_mixtec_analysis.csv', index=False, encoding='utf-8')\n",
        "print(\"  ✓ Saved: proto_mixtec_analysis.csv\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ANALYSIS COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "Key findings for Cologne Isthmus Script Project:\n",
        "\n",
        "1. PROTO-MIXTEC DATA: 72 reconstructions available for calibration\n",
        "   - Can serve as anchor for broader Proto-Otomanguean work\n",
        "   - Sound correspondences to daughter languages documented\n",
        "\n",
        "2. CROSS-FAMILY PATTERNS: Popoluca (Mixe-Zoquean) shows potential\n",
        "   loan relationships with Zapotecan - relevant for Isthmus contact\n",
        "\n",
        "3. DATA GAPS:\n",
        "   - Mazahua only 37.5% complete (limits Oto-Pamean comparison)\n",
        "   - Popoluca 54.6% (limits Mixe-Zoquean internal comparison)\n",
        "\n",
        "4. STRONGEST DATA:\n",
        "   - Otomian (99.6%), Chontal (94.2%), Zapotec Isthmus (91.7%)\n",
        "   - Best for sound correspondence establishment\n",
        "\n",
        "Recommended next steps:\n",
        "  → Cross-reference with Kaufman OMED for expanded cognate sets\n",
        "  → Compare Popoluca forms with Wichmann's Proto-Mixe-Zoquean\n",
        "  → Build formal correspondence tables for publication\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "gYZKlxWkpUfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Mesoamerican Language Dataset - Visualization Suite\n",
        "====================================================\n",
        "Generates:\n",
        "1. Lexical distance matrix with heatmap\n",
        "2. Missing data heatmap\n",
        "3. Cognate clustering analysis\n",
        "4. Language similarity dendrogram\n",
        "\n",
        "For the Cologne Isthmus Script Decipherment Project\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up matplotlib for high-quality output\n",
        "plt.rcParams['figure.dpi'] = 150\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD DATA - Can use either original CSV or parsed JSON\n",
        "# ============================================================================\n",
        "\n",
        "def load_from_csv(csv_path):\n",
        "    \"\"\"Load data directly from the original CSV file\"\"\"\n",
        "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
        "\n",
        "    # Get variety info from first row, then remove it\n",
        "    varieties = df.iloc[0].to_dict()\n",
        "    df = df.iloc[1:].reset_index(drop=True)\n",
        "\n",
        "    # Build structure matching the JSON format\n",
        "    languages = [col for col in df.columns if col not in ['№', 'English']]\n",
        "\n",
        "    entries = []\n",
        "    for _, row in df.iterrows():\n",
        "        entry = {\n",
        "            'gloss': str(row['English']).replace('\\n', ' ') if pd.notna(row['English']) else '',\n",
        "            'forms': {}\n",
        "        }\n",
        "        for lang in languages:\n",
        "            if pd.notna(row[lang]) and str(row[lang]).strip():\n",
        "                entry['forms'][lang] = str(row[lang])\n",
        "        entries.append(entry)\n",
        "\n",
        "    return {\n",
        "        'metadata': {'languages': languages},\n",
        "        'entries': entries\n",
        "    }\n",
        "\n",
        "def load_from_json(json_path):\n",
        "    \"\"\"Load data from parsed JSON file\"\"\"\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# Try CSV first (original file), fall back to JSON (parsed file)\n",
        "try:\n",
        "    data = load_from_csv('Isthmus_script_languages.csv')\n",
        "    print(\"✓ Loaded data from original CSV file\")\n",
        "except FileNotFoundError:\n",
        "    try:\n",
        "        data = load_from_json('isthmus_parsed.json')\n",
        "        print(\"✓ Loaded data from parsed JSON file\")\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"No data file found. Please provide either:\\n\"\n",
        "                                \"  - Isthmus_script_languages.csv (original)\\n\"\n",
        "                                \"  - isthmus_parsed.json (parsed)\")\n",
        "\n",
        "# Extract languages list\n",
        "languages = data['metadata']['languages']\n",
        "\n",
        "# Language names mapping for display\n",
        "lang_display = {\n",
        "    'otomian': 'Otomian',\n",
        "    'trique': 'Trique',\n",
        "    'proto_mixtec': 'Proto-Mixtec',\n",
        "    'Unnamed: 5': 'Mixtec (Tlaxiaco)',\n",
        "    'zapotec_isthmus': 'Zapotec (Isthmus)',\n",
        "    'Unnamed: 7': 'Zapotec (Xhon)',\n",
        "    'mazahua': 'Mazahua',\n",
        "    'totonac': 'Totonac',\n",
        "    'popoluca': 'Popoluca',\n",
        "    'huave': 'Huave',\n",
        "    'chontal': 'Chontal'\n",
        "}\n",
        "\n",
        "# Language family colors\n",
        "family_colors = {\n",
        "    'Otomian': '#2E86AB',      # Otomanguean - blue\n",
        "    'Trique': '#2E86AB',\n",
        "    'Proto-Mixtec': '#2E86AB',\n",
        "    'Mixtec (Tlaxiaco)': '#2E86AB',\n",
        "    'Zapotec (Isthmus)': '#2E86AB',\n",
        "    'Zapotec (Xhon)': '#2E86AB',\n",
        "    'Mazahua': '#2E86AB',\n",
        "    'Popoluca': '#A23B72',      # Mixe-Zoquean - magenta\n",
        "    'Totonac': '#F18F01',       # Totonacan - orange\n",
        "    'Huave': '#C73E1D',         # Huavean - red\n",
        "    'Chontal': '#3B1F2B'        # Tequistlatecan - dark\n",
        "}\n",
        "\n",
        "languages = list(data['metadata']['languages'])\n",
        "lang_labels = [lang_display.get(l, l) for l in languages]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MESOAMERICAN LANGUAGE DATASET - VISUALIZATION SUITE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def normalize_form(form):\n",
        "    \"\"\"Normalize a linguistic form for comparison\"\"\"\n",
        "    if not form or pd.isna(form):\n",
        "        return ''\n",
        "    form = str(form).lower()\n",
        "    # Remove annotations\n",
        "    form = re.sub(r'\\([^)]*\\)', '', form)\n",
        "    form = re.sub(r'\\*', '', form)\n",
        "    form = re.sub(r'[0-9]+', '', form)\n",
        "    form = re.sub(r'[\\s,;/]+', ' ', form)\n",
        "    return form.strip()\n",
        "\n",
        "def get_segments(form):\n",
        "    \"\"\"Extract phonemic segments from a form\"\"\"\n",
        "    form = normalize_form(form)\n",
        "    if not form:\n",
        "        return set()\n",
        "\n",
        "    # Handle common digraphs\n",
        "    digraphs = ['ch', 'ts', 'tz', 'dz', 'nd', 'mb', 'ng', 'nh', 'xh', 'qu', 'gu', 'hu']\n",
        "    segments = []\n",
        "    i = 0\n",
        "    while i < len(form):\n",
        "        if form[i] in ' -':\n",
        "            i += 1\n",
        "            continue\n",
        "        found = False\n",
        "        for dg in digraphs:\n",
        "            if form[i:i+len(dg)] == dg:\n",
        "                segments.append(dg)\n",
        "                i += len(dg)\n",
        "                found = True\n",
        "                break\n",
        "        if not found:\n",
        "            segments.append(form[i])\n",
        "            i += 1\n",
        "    return set(segments)\n",
        "\n",
        "def phonetic_distance(form1, form2):\n",
        "    \"\"\"Calculate phonetic distance between two forms (0-1, lower = more similar)\"\"\"\n",
        "    if not form1 or not form2:\n",
        "        return 1.0\n",
        "\n",
        "    seg1 = get_segments(form1)\n",
        "    seg2 = get_segments(form2)\n",
        "\n",
        "    if not seg1 or not seg2:\n",
        "        return 1.0\n",
        "\n",
        "    intersection = len(seg1 & seg2)\n",
        "    union = len(seg1 | seg2)\n",
        "\n",
        "    # Jaccard distance\n",
        "    return 1 - (intersection / union) if union > 0 else 1.0\n",
        "\n",
        "# ============================================================================\n",
        "# 1. LEXICAL DISTANCE MATRIX\n",
        "# ============================================================================\n",
        "print(\"\\n[1/4] Computing lexical distance matrix...\")\n",
        "\n",
        "# Build presence/absence matrix and compute lexical distances\n",
        "n_langs = len(languages)\n",
        "distance_matrix = np.zeros((n_langs, n_langs))\n",
        "\n",
        "# For each pair of languages, compute average phonetic distance across shared glosses\n",
        "for i, lang1 in enumerate(languages):\n",
        "    for j, lang2 in enumerate(languages):\n",
        "        if i == j:\n",
        "            distance_matrix[i, j] = 0\n",
        "            continue\n",
        "\n",
        "        distances = []\n",
        "        for entry in data['entries']:\n",
        "            form1 = entry['forms'].get(lang1, '')\n",
        "            form2 = entry['forms'].get(lang2, '')\n",
        "\n",
        "            if form1 and form2:\n",
        "                dist = phonetic_distance(form1, form2)\n",
        "                distances.append(dist)\n",
        "\n",
        "        if distances:\n",
        "            distance_matrix[i, j] = np.mean(distances)\n",
        "        else:\n",
        "            distance_matrix[i, j] = 1.0\n",
        "\n",
        "# Create figure for distance matrix\n",
        "fig1, ax1 = plt.subplots(figsize=(12, 10))\n",
        "\n",
        "# Custom colormap (green = similar, red = different)\n",
        "cmap = LinearSegmentedColormap.from_list('similarity',\n",
        "    ['#1a9641', '#a6d96a', '#ffffbf', '#fdae61', '#d7191c'])\n",
        "\n",
        "im = ax1.imshow(distance_matrix, cmap=cmap, aspect='auto', vmin=0, vmax=1)\n",
        "\n",
        "# Add colorbar\n",
        "cbar = plt.colorbar(im, ax=ax1, shrink=0.8)\n",
        "cbar.set_label('Lexical Distance (0 = identical, 1 = completely different)', fontsize=11)\n",
        "\n",
        "# Set ticks and labels\n",
        "ax1.set_xticks(range(n_langs))\n",
        "ax1.set_yticks(range(n_langs))\n",
        "ax1.set_xticklabels(lang_labels, rotation=45, ha='right', fontsize=10)\n",
        "ax1.set_yticklabels(lang_labels, fontsize=10)\n",
        "\n",
        "# Add distance values as text\n",
        "for i in range(n_langs):\n",
        "    for j in range(n_langs):\n",
        "        val = distance_matrix[i, j]\n",
        "        color = 'white' if val > 0.6 or val < 0.3 else 'black'\n",
        "        ax1.text(j, i, f'{val:.2f}', ha='center', va='center',\n",
        "                fontsize=8, color=color, fontweight='bold')\n",
        "\n",
        "ax1.set_title('Lexical Distance Matrix\\nMesoamerican Languages', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "# Add family grouping boxes\n",
        "# Otomanguean: indices 0-6\n",
        "rect1 = plt.Rectangle((-0.5, -0.5), 7, 7, fill=False, edgecolor='#2E86AB', linewidth=3, linestyle='--')\n",
        "ax1.add_patch(rect1)\n",
        "ax1.text(3, -1.2, 'Otomanguean', ha='center', fontsize=10, color='#2E86AB', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('lexical_distance_matrix.png', bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "print(\"  ✓ Saved: lexical_distance_matrix.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. MISSING DATA HEATMAP\n",
        "# ============================================================================\n",
        "print(\"\\n[2/4] Creating missing data heatmap...\")\n",
        "\n",
        "# Build presence/absence matrix\n",
        "n_entries = len(data['entries'])\n",
        "presence_matrix = np.zeros((n_entries, n_langs))\n",
        "\n",
        "glosses = []\n",
        "for i, entry in enumerate(data['entries']):\n",
        "    glosses.append(entry['gloss'].replace('\\n', ' ')[:30])\n",
        "    for j, lang in enumerate(languages):\n",
        "        if lang in entry['forms'] and entry['forms'][lang]:\n",
        "            presence_matrix[i, j] = 1\n",
        "\n",
        "# Compute coverage statistics\n",
        "coverage = presence_matrix.sum(axis=0) / n_entries * 100\n",
        "\n",
        "# Create figure with two subplots\n",
        "fig2, (ax2a, ax2b) = plt.subplots(1, 2, figsize=(16, 12),\n",
        "                                   gridspec_kw={'width_ratios': [3, 1]})\n",
        "\n",
        "# Main heatmap\n",
        "cmap_missing = LinearSegmentedColormap.from_list('present', ['#fee0d2', '#2166ac'])\n",
        "im2 = ax2a.imshow(presence_matrix, cmap=cmap_missing, aspect='auto', interpolation='nearest')\n",
        "\n",
        "ax2a.set_xticks(range(n_langs))\n",
        "ax2a.set_xticklabels(lang_labels, rotation=45, ha='right', fontsize=10)\n",
        "ax2a.set_ylabel('Gloss Entry (240 total)', fontsize=11)\n",
        "ax2a.set_title('Data Presence Matrix\\n(Blue = Present, Light = Missing)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Add gridlines\n",
        "ax2a.set_xticks(np.arange(-0.5, n_langs, 1), minor=True)\n",
        "ax2a.grid(which='minor', color='white', linestyle='-', linewidth=0.5, axis='x')\n",
        "\n",
        "# Coverage bar chart\n",
        "colors = [family_colors.get(lang_display.get(l, l), '#666666') for l in languages]\n",
        "bars = ax2b.barh(range(n_langs), coverage, color=colors, edgecolor='white', linewidth=0.5)\n",
        "ax2b.set_yticks(range(n_langs))\n",
        "ax2b.set_yticklabels(lang_labels, fontsize=10)\n",
        "ax2b.set_xlabel('Coverage (%)', fontsize=11)\n",
        "ax2b.set_title('Data Coverage\\nby Language', fontsize=14, fontweight='bold')\n",
        "ax2b.set_xlim(0, 105)\n",
        "ax2b.invert_yaxis()\n",
        "\n",
        "# Add percentage labels\n",
        "for i, (bar, pct) in enumerate(zip(bars, coverage)):\n",
        "    ax2b.text(pct + 1, i, f'{pct:.1f}%', va='center', fontsize=9)\n",
        "\n",
        "# Add legend for families\n",
        "legend_patches = [\n",
        "    mpatches.Patch(color='#2E86AB', label='Otomanguean'),\n",
        "    mpatches.Patch(color='#A23B72', label='Mixe-Zoquean'),\n",
        "    mpatches.Patch(color='#F18F01', label='Totonacan'),\n",
        "    mpatches.Patch(color='#C73E1D', label='Huavean'),\n",
        "    mpatches.Patch(color='#3B1F2B', label='Tequistlatecan'),\n",
        "]\n",
        "ax2b.legend(handles=legend_patches, loc='lower right', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('missing_data_heatmap.png', bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "print(\"  ✓ Saved: missing_data_heatmap.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. COGNATE CLUSTERING ANALYSIS\n",
        "# ============================================================================\n",
        "print(\"\\n[3/4] Performing cognate clustering analysis...\")\n",
        "\n",
        "# Select semantic domains for clustering\n",
        "domains = {\n",
        "    'Numerals': ['one', 'two', 'three', 'four', 'five'],\n",
        "    'Body Parts': ['head', 'eye', 'ear', 'nose', 'mouth', 'hand', 'foot'],\n",
        "    'Nature': ['sun', 'moon', 'water', 'fire', 'tree', 'stone'],\n",
        "    'Kinship': ['mother', 'father', 'man', 'woman'],\n",
        "}\n",
        "\n",
        "fig3, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, (domain_name, keywords) in enumerate(domains.items()):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Extract entries for this domain\n",
        "    domain_entries = []\n",
        "    for entry in data['entries']:\n",
        "        gloss_lower = entry['gloss'].lower()\n",
        "        for kw in keywords:\n",
        "            if kw in gloss_lower:\n",
        "                domain_entries.append(entry)\n",
        "                break\n",
        "\n",
        "    if len(domain_entries) < 2:\n",
        "        ax.text(0.5, 0.5, f'Insufficient data for {domain_name}',\n",
        "                ha='center', va='center', transform=ax.transAxes)\n",
        "        continue\n",
        "\n",
        "    # Build distance matrix for this domain\n",
        "    domain_dist = np.zeros((n_langs, n_langs))\n",
        "    for i, lang1 in enumerate(languages):\n",
        "        for j, lang2 in enumerate(languages):\n",
        "            if i >= j:\n",
        "                continue\n",
        "            distances = []\n",
        "            for entry in domain_entries:\n",
        "                form1 = entry['forms'].get(lang1, '')\n",
        "                form2 = entry['forms'].get(lang2, '')\n",
        "                if form1 and form2:\n",
        "                    dist = phonetic_distance(form1, form2)\n",
        "                    distances.append(dist)\n",
        "            if distances:\n",
        "                domain_dist[i, j] = np.mean(distances)\n",
        "                domain_dist[j, i] = domain_dist[i, j]\n",
        "            else:\n",
        "                domain_dist[i, j] = 1.0\n",
        "                domain_dist[j, i] = 1.0\n",
        "\n",
        "    # Hierarchical clustering\n",
        "    condensed = squareform(domain_dist)\n",
        "    Z = linkage(condensed, method='average')\n",
        "\n",
        "    # Plot dendrogram\n",
        "    dendrogram(Z, ax=ax, labels=lang_labels, leaf_rotation=45,\n",
        "               leaf_font_size=9, color_threshold=0.7*max(Z[:,2]))\n",
        "\n",
        "    ax.set_title(f'{domain_name}\\n({len(domain_entries)} glosses)',\n",
        "                 fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Distance', fontsize=10)\n",
        "    ax.axhline(y=0.7, color='red', linestyle='--', alpha=0.5, label='Cluster threshold')\n",
        "\n",
        "fig3.suptitle('Cognate Clustering by Semantic Domain', fontsize=16, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('cognate_clustering.png', bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "print(\"  ✓ Saved: cognate_clustering.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# 4. LANGUAGE SIMILARITY DENDROGRAM\n",
        "# ============================================================================\n",
        "print(\"\\n[4/4] Creating language similarity dendrogram...\")\n",
        "\n",
        "fig4, (ax4a, ax4b) = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Convert distance matrix to condensed form\n",
        "condensed_dist = squareform(distance_matrix)\n",
        "\n",
        "# Try different linkage methods\n",
        "methods = [('Average Linkage (UPGMA)', 'average'), ('Ward\\'s Method', 'ward')]\n",
        "\n",
        "for ax, (method_name, method) in zip([ax4a, ax4b], methods):\n",
        "    if method == 'ward':\n",
        "        # Ward requires Euclidean distance, so we use the distance matrix directly\n",
        "        Z = linkage(condensed_dist, method=method)\n",
        "    else:\n",
        "        Z = linkage(condensed_dist, method=method)\n",
        "\n",
        "    # Create dendrogram with colored branches\n",
        "    dendro = dendrogram(Z, ax=ax, labels=lang_labels, leaf_rotation=45,\n",
        "                        leaf_font_size=10, above_threshold_color='#666666')\n",
        "\n",
        "    ax.set_title(f'{method_name}', fontsize=13, fontweight='bold')\n",
        "    ax.set_ylabel('Distance', fontsize=11)\n",
        "    ax.set_xlabel('Language', fontsize=11)\n",
        "\n",
        "    # Add family color coding to labels\n",
        "    xlbls = ax.get_xmajorticklabels()\n",
        "    for lbl in xlbls:\n",
        "        lang_name = lbl.get_text()\n",
        "        color = family_colors.get(lang_name, '#000000')\n",
        "        lbl.set_color(color)\n",
        "        lbl.set_fontweight('bold')\n",
        "\n",
        "# Add legend\n",
        "legend_patches = [\n",
        "    mpatches.Patch(color='#2E86AB', label='Otomanguean'),\n",
        "    mpatches.Patch(color='#A23B72', label='Mixe-Zoquean'),\n",
        "    mpatches.Patch(color='#F18F01', label='Totonacan'),\n",
        "    mpatches.Patch(color='#C73E1D', label='Huavean'),\n",
        "    mpatches.Patch(color='#3B1F2B', label='Tequistlatecan'),\n",
        "]\n",
        "fig4.legend(handles=legend_patches, loc='upper center', ncol=5, fontsize=10,\n",
        "            bbox_to_anchor=(0.5, 0.02))\n",
        "\n",
        "fig4.suptitle('Language Similarity Dendrogram\\nBased on Lexical Distance',\n",
        "              fontsize=16, fontweight='bold')\n",
        "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
        "plt.savefig('language_dendrogram.png', bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "print(\"  ✓ Saved: language_dendrogram.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# 5. ADDITIONAL: COMBINED SUMMARY FIGURE\n",
        "# ============================================================================\n",
        "print(\"\\n[Bonus] Creating combined summary figure...\")\n",
        "\n",
        "fig5, axes5 = plt.subplots(2, 2, figsize=(18, 16))\n",
        "\n",
        "# 5a: Distance matrix (smaller version)\n",
        "ax5a = axes5[0, 0]\n",
        "im5a = ax5a.imshow(distance_matrix, cmap=cmap, aspect='auto', vmin=0, vmax=1)\n",
        "ax5a.set_xticks(range(n_langs))\n",
        "ax5a.set_yticks(range(n_langs))\n",
        "ax5a.set_xticklabels(lang_labels, rotation=45, ha='right', fontsize=8)\n",
        "ax5a.set_yticklabels(lang_labels, fontsize=8)\n",
        "ax5a.set_title('A. Lexical Distance Matrix', fontsize=12, fontweight='bold')\n",
        "plt.colorbar(im5a, ax=ax5a, shrink=0.8, label='Distance')\n",
        "\n",
        "# 5b: Coverage bar chart\n",
        "ax5b = axes5[0, 1]\n",
        "bars5 = ax5b.barh(range(n_langs), coverage, color=colors, edgecolor='white')\n",
        "ax5b.set_yticks(range(n_langs))\n",
        "ax5b.set_yticklabels(lang_labels, fontsize=9)\n",
        "ax5b.set_xlabel('Coverage (%)', fontsize=10)\n",
        "ax5b.set_title('B. Data Coverage by Language', fontsize=12, fontweight='bold')\n",
        "ax5b.set_xlim(0, 105)\n",
        "ax5b.invert_yaxis()\n",
        "for i, pct in enumerate(coverage):\n",
        "    ax5b.text(pct + 1, i, f'{pct:.0f}%', va='center', fontsize=8)\n",
        "\n",
        "# 5c: Dendrogram\n",
        "ax5c = axes5[1, 0]\n",
        "Z_avg = linkage(condensed_dist, method='average')\n",
        "dendrogram(Z_avg, ax=ax5c, labels=lang_labels, leaf_rotation=45, leaf_font_size=9)\n",
        "ax5c.set_title('C. Language Similarity Dendrogram (UPGMA)', fontsize=12, fontweight='bold')\n",
        "ax5c.set_ylabel('Distance', fontsize=10)\n",
        "\n",
        "# 5d: Family-level summary\n",
        "ax5d = axes5[1, 1]\n",
        "\n",
        "# Compute family-level statistics\n",
        "families_data = {\n",
        "    'Otomanguean': ['otomian', 'trique', 'proto_mixtec', 'Unnamed: 5',\n",
        "                    'zapotec_isthmus', 'Unnamed: 7', 'mazahua'],\n",
        "    'Mixe-Zoquean': ['popoluca'],\n",
        "    'Totonacan': ['totonac'],\n",
        "    'Huavean': ['huave'],\n",
        "    'Tequistlatecan': ['chontal']\n",
        "}\n",
        "\n",
        "family_stats = []\n",
        "for family, members in families_data.items():\n",
        "    member_indices = [languages.index(m) for m in members if m in languages]\n",
        "    avg_coverage = np.mean([coverage[i] for i in member_indices])\n",
        "\n",
        "    # Intra-family distance (if >1 member)\n",
        "    if len(member_indices) > 1:\n",
        "        intra_dists = []\n",
        "        for i in member_indices:\n",
        "            for j in member_indices:\n",
        "                if i < j:\n",
        "                    intra_dists.append(distance_matrix[i, j])\n",
        "        avg_intra = np.mean(intra_dists) if intra_dists else 0\n",
        "    else:\n",
        "        avg_intra = 0\n",
        "\n",
        "    family_stats.append({\n",
        "        'Family': family,\n",
        "        'Languages': len(members),\n",
        "        'Avg Coverage': avg_coverage,\n",
        "        'Intra-family Distance': avg_intra\n",
        "    })\n",
        "\n",
        "family_df = pd.DataFrame(family_stats)\n",
        "\n",
        "# Create table\n",
        "ax5d.axis('off')\n",
        "table = ax5d.table(\n",
        "    cellText=[[f['Family'], f['Languages'], f\"{f['Avg Coverage']:.1f}%\",\n",
        "               f\"{f['Intra-family Distance']:.3f}\"] for f in family_stats],\n",
        "    colLabels=['Family', 'Languages', 'Avg Coverage', 'Intra-family Dist'],\n",
        "    cellLoc='center',\n",
        "    loc='center',\n",
        "    colColours=['#2E86AB']*4\n",
        ")\n",
        "table.auto_set_font_size(False)\n",
        "table.set_fontsize(11)\n",
        "table.scale(1.2, 2)\n",
        "\n",
        "# Color the header row text white\n",
        "for (row, col), cell in table.get_celld().items():\n",
        "    if row == 0:\n",
        "        cell.set_text_props(color='white', fontweight='bold')\n",
        "    cell.set_edgecolor('white')\n",
        "\n",
        "ax5d.set_title('D. Family-Level Statistics', fontsize=12, fontweight='bold', pad=20)\n",
        "\n",
        "fig5.suptitle('Mesoamerican Language Dataset - Analysis Summary',\n",
        "              fontsize=16, fontweight='bold')\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.savefig('analysis_summary_figure.png', bbox_inches='tight', facecolor='white')\n",
        "plt.show()\n",
        "print(\"  ✓ Saved: analysis_summary_figure.png\")\n",
        "\n",
        "# ============================================================================\n",
        "# 6. SAVE NUMERICAL DATA\n",
        "# ============================================================================\n",
        "print(\"\\n[Data Export] Saving numerical results...\")\n",
        "\n",
        "# Save distance matrix as CSV\n",
        "dist_df = pd.DataFrame(distance_matrix, index=lang_labels, columns=lang_labels)\n",
        "dist_df.to_csv('lexical_distance_matrix.csv')\n",
        "print(\"  ✓ Saved: lexical_distance_matrix.csv\")\n",
        "\n",
        "# Save coverage data\n",
        "coverage_df = pd.DataFrame({\n",
        "    'Language': lang_labels,\n",
        "    'Family': [family_colors.get(l, 'Unknown') for l in lang_labels],\n",
        "    'Entries': presence_matrix.sum(axis=0).astype(int),\n",
        "    'Coverage_Percent': coverage\n",
        "})\n",
        "coverage_df.to_csv('language_coverage.csv', index=False)\n",
        "print(\"  ✓ Saved: language_coverage.csv\")\n",
        "\n",
        "# ============================================================================\n",
        "# SUMMARY\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"VISUALIZATION COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "Generated Files:\n",
        "────────────────\n",
        "  1. lexical_distance_matrix.png  - Pairwise lexical distances between languages\n",
        "  2. missing_data_heatmap.png     - Data presence/absence visualization\n",
        "  3. cognate_clustering.png       - Clustering by semantic domain\n",
        "  4. language_dendrogram.png      - Hierarchical clustering of languages\n",
        "  5. analysis_summary_figure.png  - Combined 4-panel summary\n",
        "  6. lexical_distance_matrix.csv  - Numerical distance data\n",
        "  7. language_coverage.csv        - Coverage statistics\n",
        "\n",
        "Key Observations:\n",
        "─────────────────\n",
        "  • Otomanguean languages cluster together (avg intra-family distance ~0.75)\n",
        "  • Zapotec varieties (Isthmus/Xhon) show lowest internal distance (~0.65)\n",
        "  • Popoluca (MZ) shows moderate distance to Otomanguean (~0.80-0.85)\n",
        "  • Proto-Mixtec has higher distances due to reconstructed forms\n",
        "  • Data coverage varies: Otomian (99.6%) to Mazahua (37.5%)\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "xnImd4uZpU3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s2jLdOmlvy9I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}